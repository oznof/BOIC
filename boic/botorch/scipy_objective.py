from __future__ import annotations

from collections import OrderedDict
from typing import Dict, Tuple

import numpy as np
from botorch.optim.numpy_converter import TorchAttr, set_params_with_array
from gpytorch.mlls.marginal_log_likelihood import MarginalLogLikelihood
from gpytorch.utils.errors import NanError, NotPSDError

from botorch.optim.utils import _get_extra_mll_args

# add precision argument

def _scipy_objective_and_grad(
    x: np.ndarray, mll: MarginalLogLikelihood, property_dict: Dict[str, TorchAttr],
    precision: int = None,
) -> Tuple[float, np.ndarray]:
    r"""Get objective and gradient in format that scipy expects.

    Args:
        x: The (flattened) input parameters.
        mll: The MarginalLogLikelihood module to evaluate.
        property_dict: The property dictionary required to "unflatten" the input
            parameter vector, as generated by `module_to_array`.

    Returns:
        2-element tuple containing

        - The objective value.
        - The gradient of the objective.
    """
    mll = set_params_with_array(mll, x, property_dict)
    train_inputs, train_targets = mll.model.train_inputs, mll.model.train_targets
    mll.zero_grad()
    try:  # catch linear algebra errors in gpytorch
        output = mll.model(*train_inputs)
        args = [output, train_targets] + _get_extra_mll_args(mll)
        loss = -mll(*args).sum()
    except RuntimeError as e:
        if isinstance(e, NotPSDError):
            raise e
        if isinstance(e, NanError) or "singular" in e.args[0]:
            return float("nan"), np.full_like(x, "nan")
        raise e  # pragma: nocover
    loss.backward()
    param_dict = OrderedDict(mll.named_parameters())
    grad = []
    for p_name in property_dict:
        t = param_dict[p_name].grad
        if t is None:
            # this deals with parameters that do not affect the loss
            grad.append(np.zeros(property_dict[p_name].shape.numel()))
        else:
            grad.append(t.detach().view(-1).cpu().double().clone().numpy())
    mll.zero_grad()
    loss, grad =  loss.item(), np.concatenate(grad)
    if precision is not None:
        loss = np.round(loss, precision)
        grad = np.round(grad, precision)
    #print(loss)
    return loss, grad
